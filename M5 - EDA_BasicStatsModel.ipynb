{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal=plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle=cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pywt # for deniosing time-series wavelet\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls -GFlash ../input/m5-forecasting-accuracy/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\nINPUT_DIR='../input/m5-forecasting-accuracy/'\ncal_data=pd.read_csv(INPUT_DIR+'calendar.csv')\nsalestv_data=pd.read_csv(INPUT_DIR+'sales_train_validation.csv')\nss_data=pd.read_csv(INPUT_DIR+'sample_submission.csv')\nsellp_data=pd.read_csv(INPUT_DIR+'sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salestv_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sellp_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def list_to_df(list_obj):\n    return pd.Series(list_obj)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the data for a single item that sells frequently"},{"metadata":{},"cell_type":"markdown","source":"1. Simply plotting the sales for a specific item to understand the trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ids = sorted(list(set(salestv_data['id'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_cols=[c for c in salestv_data.columns if 'd_' in c]\n\n# Below we are the chaining the following operations\n# 1. Select a particular item\n# 2. Set id as index, & keep only sales data column starting with d_\n# 3. Transform so that dataframe is a column\n# 4. Plot the data\nsalestv_data.loc[salestv_data['id']=='FOODS_3_090_CA_3_validation'] \\\n.set_index('id')[d_cols] \\\n.T \\\n.plot(figsize=(15,5), title='FOODS_3_090_CA_3 sales by \"d\" number',color=next(color_cycle))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Plotting a random sample sales data by merging with calendar data to plot exact dates on x-axis "},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = ['FOODS_3_090_CA_3_validation', 'HOBBIES_1_234_CA_3_validation', 'HOUSEHOLD_1_118_CA_3_validation', 'HOBBIES_1_001_CA_1_validation']\nexamples = []\n\n# General function to extract specific item Id values from sales_train_evaluation dataset\nfor i in range(len(ids)):\n    # Merge calendar date with sales_train_evaluation date\n    examples.append(salestv_data.loc[salestv_data['id']== ids[i]][d_cols].T) # Fetch a specific Id and transform it as column\n    examples[i] = examples[i].rename(columns={examples[i].columns.values[0] : ids[i]}) # Rename column e.g 8412 (location Id) to the proper name\n    examples[i] = examples[i].reset_index().rename(columns={'index':'d'}) # Reset index so, d_cols are not index and then, rename index as d\n    examples[i] = examples[i].merge(cal_data) # Merge two dataframe on the bases of column named d\n\n\ngraph_ids = [2]\nfig, axs = plt.subplots(len(graph_ids),1,figsize=(15,10)) \nif len(graph_ids) != 1 : axs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    # Set date as the index of dataframe to be plotted properly and only fetch specific Id sales value\n    examples[graph_id].set_index('date')[ids[graph_id]] \\\n    .plot(figsize=(15,5), color=next(color_cycle), title= f'{ids[graph_id]} sales by actual sale dates', ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    \n    ax_id+=1\nplt.tight_layout()\nplt.show()\n\n# Creating a specific example for an item Id\n#example2 = salestv_data.loc[salestv_data['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].T\n#example2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\n#example2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\n#example2 = example2.merge(cal_data, how='left', validate='1:1')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Plot the sample sale snippet, kind of zooming technique "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(graph_ids),1,figsize=(15,10)) \nif len(graph_ids) != 1 : axs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    examples[graph_id][:99][ids[graph_id]].plot(figsize=(15,5),color=next(color_cycle), marker='o',\n                                                title=f'Sampled zoomed area of {ids[graph_id]} sales', ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    ax_id+=1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Deniosing sales to know underlying trends  "},{"metadata":{},"cell_type":"markdown","source":"3.1. Wavelet denoising - calculate wavelet coefficeints and discard low coefficients "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return Mean absolute deviation value\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d-np.mean(d,axis)), axis)\n\n# Denoise the signal\ndef denoise(x, wavelet='db4',level=1):\n    coeff = pywt.wavedec(x,wavelet,mode='per')\n    sigma=(1/0.6745)*maddest(coeff[-level])\n    \n    uthresh=sigma*np.sqrt(2*np.log(len(x)))\n    coeff[1:]=(pywt.threshold(i,value=uthresh,mode='hard') for i in coeff[1:])\n    return pywt.waverec(coeff,wavelet,mode='per')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(graph_ids),1,figsize=(15,10)) \nif len(graph_ids) != 1 : axs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    examples[graph_id][ids[graph_id]][:99].plot(figsize=(15,5),color=next(color_cycle), marker='o', label='Original',\n                                                title=f'Sampled zoomed area of {ids[graph_id]} sales along with wavelet denoised trend', ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    list_to_df(denoise(examples[graph_id][ids[graph_id]][:99])).plot(figsize=(15,5),color=next(color_cycle), label='Denoised',\n                                                ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    axs[ax_id].legend() if len(graph_ids)!=1 else axs.legend()\n    ax_id+=1\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting original sales and denoised sales side-by-side for better understanding the trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(graph_ids),2,figsize=(30,10)) \naxs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    examples[graph_id][ids[graph_id]][:99].plot(figsize=(15,5),color=next(color_cycle), marker='o', alpha=0.5,\n                                                title='Original sales', ax=axs[ax_id])\n    ax_id+=1\n    list_to_df(denoise(examples[graph_id][ids[graph_id]][:99])).plot(figsize=(15,5),color=next(color_cycle), alpha=0.5,\n                                                                     title='After wavelet denoising', ax=axs[ax_id])\n    ax_id+=1\n    plt.suptitle(f'{ids[graph_id]}')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.2. Average smoothing or rolling mean denoising - A method that uses window & stride concept to denoise the data by taking mean of windowed elements and moving the window by stride "},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_smoothing(x, window=3, stride=1):\n    new_x = []\n    start = 0\n    end = window\n    while end<=len(x):\n        new_x.extend(np.ones(end-start)*np.mean(x[start:end]))\n        start = start + stride\n        end = end + stride\n    return np.array(new_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(graph_ids),1,figsize=(15,10)) \nif len(graph_ids) != 1 : axs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    examples[graph_id][ids[graph_id]][:99].plot(figsize=(15,5),color=next(color_cycle), marker='o', label='Original',\n                                                title=f'Sampled zoomed area of {ids[graph_id]} sales along with average smoothing denoised trend', ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    list_to_df(average_smoothing(examples[graph_id][ids[graph_id]])[:99]).plot(figsize=(15,5),color=next(color_cycle), label='Denoised',\n                                                ax=axs[ax_id] if len(graph_ids)!=1 else axs)\n    axs[ax_id].legend() if len(graph_ids)!=1 else axs.legend()\n    ax_id+=1\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting original sales and denoised sales side-by-side for better understanding the trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(len(graph_ids),2,figsize=(30,10)) \naxs=axs.flatten() \n\nax_id=0\nfor graph_id in graph_ids:\n    examples[graph_id][ids[graph_id]][:99].plot(figsize=(15,5),color=next(color_cycle), marker='o', alpha=0.5,\n                                                title='Original sales', ax=axs[ax_id])\n    ax_id+=1\n    list_to_df(average_smoothing(examples[graph_id][ids[graph_id]])[:99]).plot(figsize=(15,5),color=next(color_cycle), alpha=0.5,\n                                                                     title='After average smoothing denoising', ax=axs[ax_id])\n    ax_id+=1\n    plt.suptitle(f'{ids[graph_id]}')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales broken down by day of the week, month & year"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,3))\n    # Groupby wday colum and do mean or count etc. to values of other columns & then, extract the specific sales Id column  \n    examples[i].groupby('wday').mean()[ids[i]] \\\n    .plot(kind='line', title='avg sale: day of week', color=color_pal[0], ax=ax1)\n    \n    examples[i].groupby('month').mean()[ids[i]] \\\n    .plot(kind='line', title='avg sale: month', color=color_pal[1], ax=ax2)\n    \n    examples[i].groupby('year').mean()[ids[i]] \\\n    .plot(kind='line', title='avg sale: year', color=color_pal[2], ax=ax3)\n    \n    fig.suptitle(f'Trends for item: {ids[i]}', size=20, y=1.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing random 20 different items with their sales to analyze some trends"},{"metadata":{"trusted":true},"cell_type":"code","source":"twenty_examples = salestv_data.sample(20, random_state=529) # Sample random 20 samples from the salestv dataframe\nlist_of_sampled_id = list(twenty_examples['id']) # Extract the list of sampled id of items so as to just keep these id values in the dataframe eventually\ntwenty_examples = twenty_examples.set_index('id')[d_cols].T # Set the index of dataframe to be id column, select only the d columns & transform the \ntwenty_examples = twenty_examples.reset_index().rename(columns={'index':'d'})\ntwenty_examples = twenty_examples.merge(cal_data)\ntwenty_examples = twenty_examples.set_index('date')[list_of_sampled_id] \n\n\nfig, axs = plt.subplots(10,2,figsize=(20,20))\naxs = axs.flatten()\n\naxs_id=0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item, color=next(color_cycle), ax=axs[axs_id])\n    axs_id+=1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales over time by category type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of count of Items by category\nsalestv_data.groupby('cat_id').count()['id'].sort_values().plot(kind='barh', title='Count of Items by Category', figsize=(15,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales_item = salestv_data.set_index('id')[d_cols].T \\\n.merge(cal_data.set_index('d')['date'], left_index=True, right_index=True).set_index('date')\n\n#print(past_sales)\nfor i in salestv_data['cat_id'].unique():\n    items_col = [c for c in past_sales_item.columns if i in c]\n    past_sales_item[items_col].sum(axis=1).plot(figsize=(20,8), title='Total Sales by Item Type')\n\nplt.legend(salestv_data['cat_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Roll out of Items being sold"},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales_clipped = past_sales_item.clip(0,1) # Give the value of 0 or 1 to rows with no value or some value respectively, trim values to input thresholds\n\nfor i in salestv_data['cat_id'].unique():\n    items_col = [c for c in past_sales_clipped.columns if i in c]\n    (past_sales_clipped[items_col].mean(axis=1)*100).plot(figsize=(20,8), style='.', title='Inventory Sale Percentage by Date')\nplt.ylabel('% of Inventory with at least 1 sale')\nplt.legend(salestv_data['cat_id'].unique())\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales by store category"},{"metadata":{"trusted":true},"cell_type":"code","source":"#past_sales_store = salestv_data.groupby('store_id').sum().T \\\n#.merge(cal_data.set_index('d')['date'], left_index=True, right_index=True).set_index('date')\n\nstore_list = sellp_data['store_id'].unique()\nfor i in store_list:\n    # Rolling 90 day average of sales \n    store_items = [c for c in past_sales_item.columns if i in c]\n    past_sales_item[store_items].sum(axis=1).rolling(90).mean() \\\n    .plot(kind='line', figsize=(20,6), color=next(color_cycle), title='Rolling 90 Day Average Total Sales (10 stores)')\n\nplt.ylabel('Sales')\nplt.legend(store_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rolling 90 day whisker plot for average sales and store - by matplolib"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = []\n\nfor s in store_list:\n    store_items = [c for c in past_sales_item.columns if s in c]\n    data = past_sales_item[store_items].sum(axis=1).rolling(90).mean()\n    all_data.append(np.nan_to_num(np.array(data)))\n\n\nplt.figure(figsize=(20,7)) \ngreen_diamond = dict(markerfacecolor='g', marker='D')\nbplot = plt.boxplot(all_data, labels=store_list, flierprops=green_diamond, patch_artist=True, manage_ticks=True)\n\n# fill boxplots with colors\ncolors = [next(color_cycle) for i in range(0,len(store_list))]\nfor patch, color in zip(bplot['boxes'], colors):\n    patch.set_facecolor(color)   \n\nplt.xlabel('Store')\nplt.ylabel('Sales')\nplt.title('90 day rolling average sale vs store name - via matplotlib')\nplt.legend(bplot['boxes'],store_list, bbox_to_anchor=(1.07,1.02))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rolling 90 day whisker plot for average sales and store - by plotly"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfig=go.Figure()\nfor i, s in enumerate(store_list):\n    store_items = [c for c in past_sales_item.columns if s in c]\n    data = past_sales_item[store_items].sum(axis=1).rolling(90).mean()\n    fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Stores\", title=\"90 day rolling average sale vs store name - via plotly\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean sales of store "},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_value_store = []\nfor s in store_list:\n    store_items = [c for c in past_sales_item if s in c]\n    mean_value_store.append(np.mean(past_sales_item[store_items].sum(axis=1)))\n\nplt.figure(figsize=(15,7))\nbplot = plt.bar(store_list,mean_value_store)\n\n#fill different colors for bar plot\ncolors = [next(color_cycle) for i in range(0,len(store_list))]\nfor i, b in enumerate(bplot):\n    b.set_color(colors[i])\n    \nplt.xlabel('Stores')\nplt.ylabel('Mean sales')\nplt.title('Mean sales for different stores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rolling 7 day demand count by store "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(5,2,figsize=(15,10))\naxs=axs.flatten()\nax_id=0\n\nfor i in sellp_data['store_id'].unique():\n    store_items = [c for c in past_sales_item.columns if i in c]\n    past_sales_item[store_items].sum(axis=1) \\\n    .rolling(7).mean() \\\n    .plot(title=i, color=next(color_cycle), ax=axs[ax_id])\n    \n    ax_id+=1\n\nplt.suptitle('Rolling 7 day demand count by store')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salestv_data.groupby('store_id').sum().T \\\n.merge(cal_data.set_index('d'), left_index=True, right_index=True).set_index('date').groupby('wday').mean()[sellp_data['store_id'].unique()] \n\n#fig, axs = plt.subplots(5,2,figsize=(20,20))\n#axs=axs.flatten()\n\n#for i in data.columns:\n#    data[i].plot(color=next(color_cycle))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sale prices EDA"},{"metadata":{},"cell_type":"markdown","source":"1. Sale price of one of the most selling product over the years along different stores"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(15,5))\nstores=[]\n\nfor store, d in sellp_data.query('item_id == \"FOODS_3_090\"').groupby('store_id'):\n    # Merge cal_data with wm_yr_wk to plot against actual dates\n    merged_d = d.set_index('wm_yr_wk').merge(cal_data.set_index('wm_yr_wk'),left_index=True, right_index=True)\n    # Plot the sales over time \n    merged_d.plot(x='date', y='sell_price', color=next(color_cycle), ax=axs, legend=store, style='.')\n    stores.append(store)\n\nplt.legend(stores, bbox_to_anchor=(1.09,1.02))\nplt.suptitle('FOODS_3_090 sale price over time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Log distribution of sale price for different food categories "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a category column to sellp data \nsellp_data['Category'] = sellp_data['item_id'].apply(lambda x: x.split('_')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,3,figsize=(15,5))\nax_id=0\n\nfor cat, d in sellp_data.groupby('Category'):\n    # Do the log of values of sell_price columns & plot it's histogram distribution\n    d['sell_price'].apply(np.log1p).plot(kind='hist', bins=50,ax=axs[ax_id], color=next(color_cycle), title=f\"Distribution of {cat} prices\")\n    ax_id+=1\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = salestv_data[d_cols[-100:-30]]\nval_dataset = salestv_data[d_cols[-30:]]\n\nprint(val_dataset.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility function to plot graph\ndef plot_graphs(ids=[0,2], pred=[], pred_value=False):\n    fig, axs = plt.subplots(len(ids), 1, figsize=(20,5))\n    if len(ids)!=1 : axs = axs.flatten()\n    \n    if len(ids)==1:\n        axs.plot(list(train_dataset.loc[ids[0]].keys()), train_dataset.loc[ids[0]].values)\n        axs.plot(list(val_dataset.loc[ids[0]].keys()), val_dataset.loc[ids[0]].values)\n        if pred_value and pred.any():\n            axs.plot(list(val_dataset.loc[ids[0]].keys()), pred[ids[0]])\n        axs.tick_params(axis='x', rotation=90)\n    else:\n        ax_id=0\n        for i in ids:\n            axs[ax_id].plot(list(train_dataset.loc[i].keys()), train_dataset.loc[i].values)\n            axs[ax_id].plot(list(val_dataset.loc[i].keys()), val_dataset.loc[i].values)\n            if pred_value and pred.any():\n                axs[ax_id].plot(list(val_dataset.loc[i].keys()), pred[i])\n            axs[ax_id].tick_params(axis='x', rotation=90)\n            ax_id+=1\n    plt.suptitle('Plot of few items with train and validation data respectively')\n    plt.tight_layout()\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Plot the train & validation data to compare further model predictions "},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [0, 2]\nplot_graphs(ids, [], False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Naive approach - forecast the next day sales as current day sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_approach():\n    predictions=[]\n    for i in range(len(val_dataset.columns)):\n        if i==0:\n            predictions.append(train_dataset[train_dataset.columns[-1]].values)\n        else:\n            predictions.append(val_dataset[val_dataset.columns[i-1]].values)\n    predictions = np.transpose(np.array([list(row) for row in predictions]))\n    error_norm = np.linalg.norm(predictions[:3]-val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids=[0, 2]\npred, error_naive=naive_approach()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Moving average - Take past days (like 30) into consideration to predict the future values "},{"metadata":{"trusted":true},"cell_type":"code","source":"def moving_average(window=30):\n    predictions = []\n    for i in range(0, len(val_dataset.columns)):\n        if i==0:\n            predictions.append(np.mean(train_dataset[train_dataset.columns[-window:]].values, axis=1))\n        if i>0 and i<=window:\n            predictions.append(np.mean(train_dataset[train_dataset.columns[-window+i:]].values, axis=1) + np.mean(val_dataset[val_dataset.columns[:i]].values, axis=1))\n        if i>(window+1):\n            predictions.append(np.mean(val_dataset[val_dataset.columns[:i]].values, axis=1))\n    predictions=np.transpose(np.array([list(row) for row in predictions]))\n    error_norm = np.linalg.norm(predictions[:3]-val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids=[0, 2]\npred, error_moving = moving_average()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Hot Linear -  Exponential smoothing with trend but no seasonality https://www.youtube.com/watch?v=DUyZl-abnNM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom tqdm.notebook import tqdm as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def holt_smoothing():\n    predictions = []\n    for row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n        fit = Holt(row).fit(smoothing_level=0.3, smoothing_slope=0.01)\n        predictions.append(fit.forecast(30))\n    predictions=np.array(predictions).reshape(-1,30)\n    error_norm = np.linalg.norm(predictions[:3] - val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [0, 2]\npred, error_holt= holt_smoothing()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Exponential smoothing or holt-Winter smoothing - This take both trend and seasonality into consideration https://www.youtube.com/watch?v=mrLiC1biciY"},{"metadata":{"trusted":true},"cell_type":"code","source":"def winter_smoothing():\n    predictions = []\n    for row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n        fit = ExponentialSmoothing(row, seasonal_periods=3).fit()\n        predictions.append(fit.forecast(30))\n    predictions = np.array(predictions).reshape(-1, 30) # Reshaping to make sure it has 30 columns\n    error_norm = np.linalg.norm(predictions[:3] - val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [0, 2]\npred, error_winter = winter_smoothing()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. ARIMA model - Auto Regressive Integrated Moving Average, aim to describe correlations in time series https://www.youtube.com/watch?v=2XGSIlgUBDI"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing for stationarity using dickey fuller's test\nfrom statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def adfuller_test(val):\n    result = adfuller(val)\n    labels = ['ADF test statistics', 'p-value', '#Lags used', 'No of observations used']\n    for value, label in zip(result, labels):\n        print(label + ':' + str(value))\n    if result[1] <= 0.05:\n        print('Stationary')\n    else:\n        print('Non-stationary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adfuller_test(train_dataset[train_dataset.columns[-30:]].values[2:3][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Differencing to make it stationary\nexample = (train_dataset[train_dataset.columns[-30:]].loc[2]).reset_index()\nexample.columns = ['index', 'sales']\n\n# Number by which the seasonality is present, if the seasonal cycle is 1 cycle in 12 months then, we pass 12 \nexample['seasonal first difference'] = example['sales'] - example['sales'].shift(7) #For us it is 7 \n#print(example)\nadfuller_test(example['seasonal first difference'].dropna())\nexample['seasonal first difference'].plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Auto-correlation and partial auto-correlation to determine the lags as in how many previous days data I need to consider to make good predictions"},{"metadata":{},"cell_type":"markdown","source":"p,d,q - p for AR model lags, d for differencing, q for MA lags  \nAR model lags are best estimated with PACF, and MA model is best done with ACF"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nax1=fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(example['seasonal first difference'],lags=29,ax=ax1)\nax2=fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(example['seasonal first difference'],lags=29,ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def arima():\n    predictions = []\n    for row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n        fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0,1,1,7)).fit()\n        predictions.append(fit.forecast(30))\n    predictions = np.array(predictions).reshape(-1, 30)\n    error_norm = np.linalg.norm(predictions[:3]-val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [2]\npred, error_arima = arima()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. facebook prophet - additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, including holiday effects https://www.youtube.com/watch?v=95-HMzxsghY "},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prophet():\n    predictions = []\n    forecasts = []\n    models = []\n    dates = [\"2007-12-\" + str(i) for i in range(1, 31)]\n    for row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n        df = pd.DataFrame(np.transpose([dates, row]))\n        df.columns = ['ds', 'y']\n        model = Prophet(daily_seasonality=True)\n        model.fit(df)\n        future = model.make_future_dataframe(periods=30)\n        forecast = model.predict(future)\n        forecasts.append(forecast)\n        models.append(model)\n        predictions.append(forecast['yhat'].loc[30:].values)\n    predictions = np.array(predictions).reshape(-1,30)\n    error_norm = np.linalg.norm(predictions[:3] - val_dataset[:3]/len(predictions[0]))\n    return predictions, float(error_norm), forecasts, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [2]\npred, error_prophet, forecasts, models = prophet()\nplot_graphs(ids, pred, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting prophet forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = models[2]\nforecast2 = forecasts[2]\n\nmodel2.plot(forecast2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting forecast componenets"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.plot_components(forecast2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot_plotly(model2, forecast2)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Error Loss of all models "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_axis = ['Naive', 'Moving Average', 'Holt linear', 'Holt-Winter', 'ARIMA', 'Prophet']\ny_axis = [error_naive, error_moving, error_holt, error_winter, error_arima, error_prophet]\nloss_df = pd.DataFrame(np.transpose([x_axis, y_axis]))\nloss_df.columns = ['model', 'error']\nfor i in range(6):\n    loss_df['error'][i] = float(loss_df['error'][i])\n\nplt.figure(figsize=(15,5))\nbplot = plt.bar(x_axis, loss_df['error'])\n\n#fill different colors for bar plot\ncolors = [next(color_cycle) for i in range(0,6)]\nfor i, b in enumerate(bplot):\n    b.set_color(colors[i])\n    \nplt.xlabel('Models')\nplt.ylabel('Error loss')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}