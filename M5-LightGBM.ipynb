{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom datetime import datetime, timedelta\nimport gc\nimport lightgbm as lgb\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_COL_TYPES = {'event_name_1':'category', 'event_type_1':'category','event_name_2':'category', 'event_type_2':'category', \n            'weekday':'category', 'wm_yr_wk': 'int16', 'wday': 'int16', 'month': 'int16', 'year':'int16', \n            'snap_CA': 'float32', 'snap_TX': 'float32', 'snap_WI':'float32'}\nSELLP_COL_TYPES = {'store_id':'category', 'item_id': 'category', 'wm_yr_wk': 'int16', 'sell_price':'float32'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read the data\nINPUT_DIR='../input/m5-forecasting-accuracy/'\ncal_data=pd.read_csv(INPUT_DIR+'calendar.csv', dtype=CAL_COL_TYPES)\nsellp_data=pd.read_csv(INPUT_DIR+'sell_prices.csv', dtype=SELLP_COL_TYPES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cal_data.shape)\ncal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_data.loc[1913:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sellp_data.shape)\nsellp_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = 28\nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4,25)\nfday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sellp_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_df(is_train=True, nrows=None, first_day=1200):\n    # convert categorical colum values to numerical for sell_prices.csv \n    sellp_data_change = sellp_data.copy()\n    for col, col_dtype in SELLP_COL_TYPES.items():\n        if col_dtype == 'category':\n            sellp_data_change[col] = sellp_data_change[col].cat.codes.astype('int16')\n            sellp_data_change[col] -= sellp_data_change[col].min()\n    \n    cal_data_change = cal_data.copy()\n    cal_data_change['date'] = pd.to_datetime(cal_data_change['date'])\n    # convert categorical colum values to numerical for calendar.csv\n    for col, col_dtype in CAL_COL_TYPES.items():\n        if col_dtype == 'category':\n            cal_data_change[col] = cal_data_change[col].cat.codes.astype('int16')\n            cal_data_change[col] -= cal_data_change[col].min()\n            \n    start_day = max(1 if is_train else tr_last-max_lags, first_day)\n    dcols = [f'd_{day}' for day in range(start_day, tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    dtype = {dcol : 'float32' for dcol in dcols}\n    dtype.update({col:'category' for col in catcols if col!='id'})\n    salestv_data=pd.read_csv(INPUT_DIR+'sales_train_validation.csv', nrows=nrows, usecols= catcols+dcols, dtype=dtype)\n    \n    # convert categorical colum values to numerical for sales_train_validation.csv \n    for col in catcols:\n        if col != 'id':\n            salestv_data[col] = salestv_data[col].cat.codes.astype('int16')\n            salestv_data[col] -= salestv_data[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+28+1):\n            salestv_data[f'd_{day}'] = np.nan\n    \n    # Unpivot the datafame along d_cols\n    df = pd.melt(salestv_data, \n                 id_vars=catcols, \n                 value_vars=[col for col in salestv_data.columns if col.startswith('d_')],\n                 var_name='d',\n                 value_name='sales'\n                )\n    df = df.merge(cal_data_change, on='d', copy=False)\n    df = df.merge(sellp_data_change, on=['store_id', 'item_id', 'wm_yr_wk'], copy=False)\n    return df        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\n#df_test = create_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(df_test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(df):\n    lags =[7, 28]\n    lag_cols = [f'd_{lag}' for lag in lags]\n    \n    # shift the sales by lag value and append a new column\n    for lag, lag_col in zip(lags, lag_cols):\n        df[lag_col] = df[['id', 'sales']].groupby('id')['sales'].shift(lag)\n        \n    wins = [7, 28]\n    for win in wins:\n        for lag, lag_col in zip(lags, lag_cols):\n            df[f'rmean_{lag}_{win}'] = df[['id', lag_col]].groupby('id')[lag_col].transform(lambda x: x.rolling(win).mean())\n    \n    date_features = {\n        'wday':'weekday',\n        'week':'weekofyear',\n        'month':'month',\n        'quarter':'quarter',\n        'year':'year',\n        'mday':'day'\n    }\n    \n    for date_feature_name, date_feature_func in date_features.items():\n        if date_feature_name in df.columns:\n            df[date_feature_name] = df[date_feature_name].astype('int16')\n        else:\n            df[date_feature_name] = getattr(df['date'].dt, date_feature_func).astype('int16')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FIRST_DAY=1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sellp_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf = create_df(is_train=True, first_day = FIRST_DAY)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncreate_fea(df)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop missing value rows\ndf.dropna(inplace=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sellp_data, cal_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['item_id', 'store_id', 'cat_id', 'dept_id', 'state_id'] + ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nuseless_cols = ['id', 'date', 'sales', 'd', 'wm_yr_wk', 'weekday']\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df['sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nnp.random.seed(777)\n\n# This is a random sample, we're not gonna apply any time series train-test-split tricks here!\nfake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace=False) # Validation dataset\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds) # Training dataset\n\ntrain_data = lgb.Dataset(X_train.loc[train_inds], label=y_train.loc[train_inds], categorical_feature=cat_features, free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label=y_train.loc[fake_valid_inds], categorical_feature=cat_features, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB parameters\n\nparams={\n#    'device':'gpu',\n    'objective':'poisson',\n    'metric':['rmse'],\n    'force_row_wise':True,\n    'learning_rate':0.075,\n    'sub_row': 0.75,\n    'bagging_freq': 1,\n    'lambda_12':0.1,\n    'verbosity':1,\n    'num_iterations':1200,\n    'num_leaves':2**11-1,\n    'min_data_in_leaf':2**12-1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nm_lgb = lgb.train(params, train_data, valid_sets=[fake_valid_data], verbose_eval=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_lgb.save_model(\"model.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nalphas = [1.028, 1.023, 1.018]\nweights = [1/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_df(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_fea(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)\n\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}